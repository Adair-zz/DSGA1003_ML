{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read both positive and negative reviews and put it into a large python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_dir = '../data/neg/'\n",
    "pos_dir = '../data/pos/'\n",
    "data = []\n",
    "step = 0\n",
    "for filename in os.listdir(neg_dir):\n",
    "#     step+=1\n",
    "#     if step>2:\n",
    "#         break\n",
    "    path = neg_dir+filename\n",
    "    with open(path) as f:\n",
    "        text = f.read().replace('\\n','')\n",
    "        example = {}\n",
    "        example['label'] = 0\n",
    "        # Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', text)\n",
    "        example['text'] = text[1:]\n",
    "        data.append(example)\n",
    "        \n",
    "for filename in os.listdir(pos_dir):\n",
    "#     step+=1\n",
    "#     if step>2:\n",
    "#         break\n",
    "    path = pos_dir+filename\n",
    "    with open(path) as f:\n",
    "        text = f.read().replace('\\n','')\n",
    "        example = {}\n",
    "        example['label'] = 1\n",
    "        # Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', text)\n",
    "        example['text'] = text[1:]\n",
    "        data.append(example)\n",
    "\n",
    "#Shuffle data\n",
    "random.seed(1)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training-validation split\n",
    "training_set = data[:1500]\n",
    "validation_set = data[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/newpage\n",
    "# Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feature extract function transfer the text into both sparse representation dictionary(named'feature') as described in question and bag-of-words representation(named 'vector') numpy vector(with tons of zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a function to extract bag-of-words features\n",
    "#The feature_function is modified from Samuel Bowman's Natural Language Processing class in fall 2016.\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, feature_indices,dim\n",
    "    \n",
    "indices_to_features,feature_indices,dim = feature_function([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-eb76a75539d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# 6 Support Vector Machine Via Pegasos\n",
    "## 6.1\n",
    "Since $\\frac{\\lambda}{2}\\lvert\\lvert w\\rvert\\rvert^2$ is convex and the hinge loss is convex, using the fact provided in the question the subgradient of the objective function is:$\n",
    "\\begin{cases}\n",
    "\\lambda w -y_i x_i & 1-y_i w x_i >0\\\\\n",
    "\\lambda w & else\n",
    "\\end{cases}\n",
    "$\n",
    "Using the stochastic subgradient descent algorithm, the $w$ is updated by:\n",
    "$\n",
    "\\begin{cases}\n",
    "w=w-\\eta_t(\\lambda w -y_i x_i)=(1-\\eta_t\\lambda)w+\\eta_ty_ix_i & 1-y_i w x_i >0\\\\\n",
    "w=w-\\eta\\lambda w= (1-\\eta_t\\lambda)w& else\n",
    "\\end{cases}\n",
    "$. This is the same as Pegasos algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## 6.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pegasos_vector(data,lambda_reg=0.1):\n",
    "    t=0\n",
    "    w=np.zeros(len(data[0]['vector'])) \n",
    "    for example in data:\n",
    "        t+=1\n",
    "        eta=1.0/(lambda_reg*t)\n",
    "        y_i = example['label']\n",
    "        x_i = example['vector']\n",
    "        if y_i*np.dot(w,x_i)<1:\n",
    "            w=(1-eta*lambda_reg)*w+eta*y_i*x_i\n",
    "        else:\n",
    "            w=(1-eta*lambda_reg)*w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_w = pegasos(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## 6.3\n",
    "Notice: With my feature extraction method, the sparse representation vector is stored as numpy array rather than dictionary. Hence my pegasos DOES NOT suffer the slow down as described in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First verify that this is equivalent.\n",
    "$$\n",
    "\\begin{split}\n",
    "W_{t+1}&=(1-\\eta_t \\lambda)W_t+\\frac{\\eta_t y_i x_i}{1-\\eta_t \\lambda}\n",
    "\\end{split}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
