{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read both positive and negative reviews and put it into a large python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_dir = '../data/neg/'\n",
    "pos_dir = '../data/pos/'\n",
    "data = []\n",
    "step = 0\n",
    "for filename in os.listdir(neg_dir):\n",
    "#     step+=1\n",
    "#     if step>2:\n",
    "#         break\n",
    "    path = neg_dir+filename\n",
    "    with open(path) as f:\n",
    "        text = f.read().replace('\\n','')\n",
    "        example = {}\n",
    "        example['label'] = -1\n",
    "        # Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', text)\n",
    "        example['text'] = text[1:]\n",
    "        data.append(example)\n",
    "        \n",
    "for filename in os.listdir(pos_dir):\n",
    "#     step+=1\n",
    "#     if step>2:\n",
    "#         break\n",
    "    path = pos_dir+filename\n",
    "    with open(path) as f:\n",
    "        text = f.read().replace('\\n','')\n",
    "        example = {}\n",
    "        example['label'] = 1\n",
    "        # Strip out the parse information and the phrase labels---we don't need those here\n",
    "        text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', text)\n",
    "        example['text'] = text[1:]\n",
    "        data.append(example)\n",
    "\n",
    "#Shuffle data\n",
    "random.seed(1)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training-validation split\n",
    "training_set = data[:1500]\n",
    "validation_set = data[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/newpage\n",
    "# Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feature extract function transfer the text into both sparse representation dictionary(named'feature') as described in question and bag-of-words representation(named 'vector') numpy vector(with tons of zeros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a function to extract bag-of-words features\n",
    "#The feature_function is modified from Samuel Bowman's Natural Language Processing class in fall 2016.\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def feature_function(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter])\n",
    "                                \n",
    "    feature_names = set()\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['features'] = collections.defaultdict(float)\n",
    "            \n",
    "            # Extract features (by name) for one example\n",
    "            word_counter = collections.Counter(tokenize(example['text']))\n",
    "            for x in word_counter.items():\n",
    "                if x[0] in vocabulary:\n",
    "                    example[\"features\"][x[0]] = x[1]\n",
    "            \n",
    "            feature_names.update(example['features'].keys())\n",
    "                            \n",
    "    # By now, we know what all the features will be, so we can\n",
    "    # assign indices to them.\n",
    "    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n",
    "    indices_to_features = {v: k for k, v in feature_indices.items()}\n",
    "    dim = len(feature_indices)\n",
    "                \n",
    "    # Now we create actual vectors from those indices.\n",
    "    for dataset in datasets:\n",
    "        for example in dataset:\n",
    "            example['vector'] = np.zeros((dim))\n",
    "            for feature in example['features']:\n",
    "                example['vector'][feature_indices[feature]] = example['features'][feature]\n",
    "    return indices_to_features, feature_indices,dim\n",
    "    \n",
    "indices_to_features,feature_indices,dim = feature_function([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "# 6 Support Vector Machine Via Pegasos\n",
    "## 6.1\n",
    "Since $\\frac{\\lambda}{2}\\lvert\\lvert w\\rvert\\rvert^2$ is convex and the hinge loss is convex, using the fact provided in the question the subgradient of the objective function is:$\n",
    "\\begin{cases}\n",
    "\\lambda w -y_i x_i & 1-y_i w x_i >0\\\\\n",
    "\\lambda w & else\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "Using the stochastic subgradient descent algorithm, the $w$ is updated by:\n",
    "$\n",
    "\\begin{cases}\n",
    "w=w-\\eta_t(\\lambda w -y_i x_i)=(1-\\eta_t\\lambda)w+\\eta_ty_ix_i & 1-y_i w x_i >0\\\\\n",
    "w=w-\\eta\\lambda w= (1-\\eta_t\\lambda)w& else\n",
    "\\end{cases}\n",
    "$. This is the same as Pegasos algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## 6.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pegasos_vector(data,lambda_reg=0.1,max_epochs=1000):\n",
    "    epoch=0\n",
    "    t=0\n",
    "    w=np.zeros(len(data[0]['vector'])) \n",
    "    while t<max_steps:\n",
    "        epoch+=1\n",
    "        for example in data:\n",
    "            t+=1.\n",
    "            eta=1.0/(lambda_reg*t)\n",
    "            y_i = example['label']\n",
    "            x_i = example['vector']\n",
    "            if y_i*np.dot(w,x_i)<1:\n",
    "                w=(1-eta*lambda_reg)*w+eta*y_i*x_i\n",
    "            else:\n",
    "                w=(1-eta*lambda_reg)*w\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pegasos_dict(data,lambda_reg=0.1,max_epochs=1000):\n",
    "    epoch=0\n",
    "    t=1.\n",
    "    w = collections.defaultdict(float)\n",
    "    while epoch<max_epochs:\n",
    "        print('step',str(t))\n",
    "        epoch+=1\n",
    "        for example in data:\n",
    "            t+=1\n",
    "            eta=1.0/(lambda_reg*t)\n",
    "            y_i = example['label']\n",
    "            x_i = example['features']\n",
    "            w_xi = 0\n",
    "            ##get w.x_i\n",
    "            for feature in x_i:\n",
    "                w_xi += x_i[feature]*w[feature]\n",
    "            #multiply every w element with (1-eta*lambda_reg)\n",
    "            w.update((x, y*(1-eta*lambda_reg)) for x, y in w.items())\n",
    "            if y_i*w_xi<1:\n",
    "                for feature in x_i:\n",
    "                    w[feature] += eta*y_i*x_i[feature]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## 6.3\n",
    "Notice: With my feature extraction method, the sparse representation vector is stored as numpy array rather than dictionary. Hence my pegasos DOES NOT suffer the slow down as described in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First verify that this is equivalent.\n",
    "$$\n",
    "\\begin{split}\n",
    "w_{t+1}&=s_{t+1}W_{t+1}\\\\\n",
    "&=(1-\\eta_t \\lambda)s_tW_t+\\eta_t y_i x_i\\\\\n",
    "&=(1-\\eta_t \\lambda)w_t+\\eta_t y_i x_i\n",
    "\\end{split}.\n",
    "$$\n",
    "The following is the implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pegasos_dict_accelerated(data,lambda_reg=0.1,max_epochs=1000):\n",
    "    epoch=1 # Want the first t to start with 2\n",
    "    W = collections.defaultdict(float)\n",
    "    s_t = 1.\n",
    "    t=1.\n",
    "    while epoch<max_epochs:\n",
    "        epoch+=1\n",
    "        for example in data:\n",
    "            t+=1\n",
    "            eta=1.0/(lambda_reg*t)\n",
    "            s_t =(1-eta*lambda_reg)*s_t\n",
    "            y_i = example['label']\n",
    "            x_i = example['features']\n",
    "            w_xi = 0\n",
    "            ##get w.x_i\n",
    "            for feature in x_i:\n",
    "                w_xi += x_i[feature]*W[feature]\n",
    "            if y_i*w_xi<1:\n",
    "                for feature in x_i:\n",
    "                    W[feature] += (1/s_t)*y_i*x_i[feature]\n",
    "        #multiply s_t and W to get w_t\n",
    "        W.update((x, y*s_t) for x, y in W.items())\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\newpage\n",
    "## 6.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1.0\n",
      "step 1501.0\n"
     ]
    }
   ],
   "source": [
    "# First Verify that 2 method yields the same result\n",
    "test_w = pegasos_dict(training_set,max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_w_acc = pegasos_dict_accelerated(training_set,max_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.31645569620253, 0.019993335554815136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_w_acc['fame'],test_w['fame']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/newpage\n",
    "## 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(w,data):\n",
    "    correct = 0\n",
    "    for example in data:\n",
    "        label = example['label']\n",
    "        w_xi=0\n",
    "        x_i = example['features']\n",
    "        w_xi = 0\n",
    "        ##get w.x_i\n",
    "        for feature in x_i:\n",
    "            w_xi += x_i[feature]*W[feature]\n",
    "        if w_xi<0:\n",
    "            prediction = -1\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if label ==prediction:\n",
    "            correct+=1\n",
    "    return correct/len(data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
